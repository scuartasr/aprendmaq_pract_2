{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Practica 2: Convolutional Neural Network (CNN)\n",
        "*Elaborado por:\n",
        "\n",
        "Luis Fernando Becerra, BEDA Aprendizaje de Máquinas 2024-1S - 2025-1S\n",
        "\n",
        "Andres Esteban Marin Manco, BEDA Aprendizaje de Máquinas 2025-1S*"
      ],
      "metadata": {
        "id": "_UbrFjp3MjPI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <b>Clasificación de Imágenes con Redes Neuronales Convolucionales (CNN)</b></font>\n",
        "\n",
        "Esta guía muestra paso a paso cómo entrenar una **Red Neuronal Convolucional (CNN)** para resolver un problema clásico de visión por computador: la **clasificación de imágenes**.\n",
        "\n",
        "El objetivo es utilizar una CNN para clasificar imágenes del conjunto de datos **CIFAR-10**, el cual contiene 60,000 imágenes a color, de tamaño 32x32 píxeles, distribuidas en 10 categorías como: aviones, gatos, automóviles, perros, etc.\n",
        "\n",
        "<br>\n",
        "\n",
        "## <b>¿Cómo lo haremos?</b></font>\n",
        "\n",
        "Usaremos la **API Secuencial de Keras**, una interfaz de alto nivel provista por TensorFlow que permite construir modelos de manera muy intuitiva. Gracias a esta API, podemos:\n",
        "\n",
        "- **Definir la arquitectura de la red** en unas pocas líneas de código.\n",
        "- **Compilar y entrenar el modelo** de forma eficiente.\n",
        "- **Evaluar el desempeño del modelo** con funciones predefinidas.\n",
        "- **Visualizar resultados** fácilmente con herramientas como Matplotlib.\n",
        "\n",
        "<br>\n",
        "\n",
        "## <b>¿Qué aprenderás?</b></font>\n",
        "A lo largo de esta guía aprenderás a:\n",
        "\n",
        "- Cargar y preprocesar datos de imágenes.\n",
        "- Construir una arquitectura CNN desde cero.\n",
        "- Entrenar el modelo y monitorear su rendimiento.\n",
        "- Evaluar los resultados obtenidos y realizar predicciones.\n",
        "\n",
        "<br>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i0.wp.com/developersbreach.com/wp-content/uploads/2020/08/cnn_banner.png?fit=1400%2C658&ssl=1\" width=\"800\"/>\n",
        "</p>\n",
        "\n"
      ],
      "metadata": {
        "id": "OF_RINziMNwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importar librerias"
      ],
      "metadata": {
        "id": "9J5daY-JMNJ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9kwXw6hKSlq"
      },
      "outputs": [],
      "source": [
        "##\n",
        "import matplotlib.pyplot as plt\n",
        "####"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Descarga y preparación del conjunto de datos CIFAR-10</b></font>\n",
        "\n",
        "El conjunto de datos **CIFAR-10** contiene **60.000 imágenes en color**, de tamaño **32x32 píxeles**, distribuidas en **10 clases diferentes**, como aviones, gatos, automóviles, camiones, entre otros.\n",
        "\n",
        "Cada clase tiene exactamente **6.000 imágenes**. El conjunto de datos se divide en:\n",
        "\n",
        "- **50.000 imágenes de entrenamiento**\n",
        "- **10.000 imágenes de prueba**\n",
        "\n",
        "Todas las clases son **mutuamente excluyentes**, lo que significa que cada imagen pertenece a una única categoría."
      ],
      "metadata": {
        "id": "kPBXGNFJOgXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Cargar los datos en memoria</b></font>\n",
        "\n",
        "Puedes cargar el conjunto de datos directamente desde la API de Keras usando la siguiente instrucción:\n",
        "\n"
      ],
      "metadata": {
        "id": "BoOlv5aXqGMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Cargar datos entrenamiento y prueba\n",
        "###"
      ],
      "metadata": {
        "id": "myoYtum3O9E_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Un paso importante de preprocesamiento de los datos a usar con redes profundas es la normalización. Este paso ayuda a los optimizadores a converger más rapido, sin que ninguna de las caracteristicas tenga mayor peso en la selección de la dirección de la convergencia. En el caso de las imágenes en escala de grises y RGB, la normalización es muy sencilla. Usualmente las imágenes en escala de grises (1 sola capa) o RGB (3 capas correspondientes a los colores rojo, verde y azul) se encuentran en formato UINT8, es decir, que cada prixel se representa por valores de 0 a 255. Por lo tanto, para la normalización de los datos solo requerimos dividir por 255.\n",
        "\n",
        "Nota: cuando se trabajan con otros tipos de imágenes (por ejemplo satelitales) es importante revisar el formato de los datos, o realizar una normalización MIN-MAX."
      ],
      "metadata": {
        "id": "mBe-RUOWPf0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalización de los datos\n",
        "###"
      ],
      "metadata": {
        "id": "z2rYJvozPmEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Verificar los datos</b></font>\n",
        "\n",
        "Para asegurarnos de que el conjunto de datos se ha cargado correctamente, visualizamos las **primeras 25 imágenes de entrenamiento** junto con su **etiqueta(nombre) de clase**.\n",
        "\n",
        "Esto nos permite:\n",
        "- Confirmar que las imágenes están bien formateadas.\n",
        "- Ver que las etiquetas corresponden correctamente a su clase.\n",
        "\n",
        "Cada imagen se muestra en una cuadrícula de 5x5 con su nombre de clase debajo."
      ],
      "metadata": {
        "id": "WNGg6yskP0ra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualización de los datos\n",
        "#Nombre de las clases\n",
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "#Visualización de las imagenes\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i])\n",
        "    # The CIFAR labels happen to be arrays,\n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "c_bvDl8OP-Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Crear la arquitectura"
      ],
      "metadata": {
        "id": "PfiOnCQYQw3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a definir una arquitectura común usando una pila de capas convolucionales (Conv2D) y de pooling (MaxPooling2D).\n",
        "\n",
        "Como entrada, una CNN toma tensores de la forma pixeles_por_fila X pixels_por_columna X no_canales (image_height, image_width, color_channels). Las imágenes en escala de grises solo cuentan con 1 canal, en cambio las imágenes que capturamos con nuestros celulares o camaras convencionales usualmente tienen 3 canales correspondientes al RGB.\n",
        "\n",
        "En nuestro ejemplo la dimesión de los datos es de 32x32x3. Con estas dimensiones configuramos la capa de entrada."
      ],
      "metadata": {
        "id": "Q5O_CFVBRL5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Definición del modelo\n",
        "\n",
        "#Capa convolucional y pooling\n",
        "\n",
        "#Capa convolucional y pooling\n",
        "\n",
        "#Capa convolucional\n"
      ],
      "metadata": {
        "id": "6bTVqeRqRdmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recordemos que con el metodo summary() podemos revisar la arquitectura de nuestra red, así como ver el número de parámetros que debemos entrenar."
      ],
      "metadata": {
        "id": "7PNzO4IYRk8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "hWlJeDWYRxEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el resumen del modelo deberias ver que la salida de cada capa Conv2D y MaxPooling2D es un tensor 3D de forma (altura, anchura, canales). Las dimensiones de anchura y altura tienden a reducirse a medida que se profundiza en la red, esto por el efecto del pooling. El número de canales de salida para cada capa Conv2D se controla mediante el primer argumento (por ejemplo, 32 o 64). Normalmente, a medida que la anchura y la altura se reducen, puedes permitirte (computacionalmente) añadir más canales de salida en cada capa Conv2D."
      ],
      "metadata": {
        "id": "2Rr9yDzZR8MD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para completar el modelo, vamos a introducir el último tensor de salida (de la forma (4, 4, 64)) en una o más capas Densas para realizar la clasificación. Las capas densas toman vectores como entrada (que son 1D), mientras que la salida actual es un tensor 3D. Por lo cual, es nesario en primer lugar \"aplanar\" el tensor (Flatten()), y a continuación, añadir una o más capas densas. CIFAR tiene 10 clases de salida, por lo que se utiliza una capa Densa final con 10 salidas.\n",
        "\n",
        "Recuerda: El número de capas convolucionales y densas, así como el número de filtros convolucionales, tipo de pooling, número de neuronas ocultas, funciones de activación, son parametros que debe determinar el ingeniero ML."
      ],
      "metadata": {
        "id": "0SuJkRksSfZ8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualiza la arquitectura completa de tu modelo:"
      ],
      "metadata": {
        "id": "rAc4tcQgTJsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model,\n",
        "           show_shapes=True,\n",
        "           show_layer_names=True,\n",
        "           dpi=50,\n",
        "           #rankdir=\"LR\" #LR:plotea horizontal, TB:plotea hacia abajo (por defecto)\n",
        ")"
      ],
      "metadata": {
        "id": "LEePd_sMTPBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b>Entendiendo el resumen del modelo</b></font>\n",
        "\n",
        "En el resumen del modelo (`model.summary()`), puedes observar que la salida de cada capa `Conv2D` y `MaxPooling2D` es un **tensor 3D** con forma:\n",
        "\n",
        "\n",
        "| Capa             | Tipo          | Forma de salida          | Parámetros |\n",
        "|------------------|---------------|---------------------------|------------|\n",
        "| `conv2d`         | Conv2D        | (None, 30, 30, 32)        | 896        |\n",
        "| `max_pooling2d`  | MaxPooling2D  | (None, 15, 15, 32)        | 0          |\n",
        "…\n",
        "<br\n",
        "\n",
        "## ¿Qué ocurre a medida que profundizamos en la red?</b></font>\n",
        "\n",
        "- **Alto y ancho**: Tienden a **disminuir** por efecto de las convoluciones y el `MaxPooling`.\n",
        "- **Número de canales (depth)**: Se **incrementa** progresivamente para capturar más características complejas.\n",
        "\n",
        "Esto responde a una práctica común: a medida que reducimos el tamaño espacial de las imágenes (menos ancho y alto), podemos **aumentar la profundidad** (más filtros) sin que el costo computacional se dispare.\n",
        "\n",
        "Por ejemplo:\n",
        "- Una capa `Conv2D(32, ...)` genera 32 mapas de características.\n",
        "- Una capa `Conv2D(64, ...)` genera 64 mapas, lo cual permite aprender representaciones más ricas.\n",
        "\n",
        "<br>\n",
        "\n",
        "Este patrón (reducir resolución y aumentar profundidad) es típico en arquitecturas CNN eficientes y escalables.\n"
      ],
      "metadata": {
        "id": "5ZdlKotcrdxY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Añadir capas densas al modelo</b></font>\n",
        "\n",
        "Para completar el modelo, debemos **transformar la salida de la base convolucional** en una representación que pueda ser interpretada por una capa de clasificación.\n",
        "\n",
        "Actualmente, la salida es un tensor de forma **(4, 4, 64)**, es decir, un volumen tridimensional.  \n",
        "Las **capas densas (`Dense`) requieren vectores 1D**, así que primero debemos **aplanar** este tensor con una capa `Flatten`.\n",
        "\n",
        "<br>\n",
        "\n",
        "## <b>Estructura final</b></font>\n",
        "\n",
        "1. **`Flatten`**: Convierte (4, 4, 64) → 1024 unidades.\n",
        "2. **`Dense(64)`**: Capa oculta totalmente conectada con activación ReLU.\n",
        "3. **`Dense(10)`**: Capa de salida con 10 unidades (una por cada clase del CIFAR-10).\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "_<font color=\"purple\"><b>Imagen de referencia de transformación a capa flattened</b></font>_\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://towardsdatascience.com/wp-content/uploads/2022/06/1N74xher1f5gJSHY-_o_rQQ.png\" width=\"200\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "TsX-Y66cvZFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Codigo para agregar capas\n",
        "##"
      ],
      "metadata": {
        "id": "ZDKBwezRvqmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualiza la arquitectura completa de tu modelo</b></font>"
      ],
      "metadata": {
        "id": "AKgg_lYYwC8Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "K-ELbbp5v7Iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model,\n",
        "           show_shapes=True,\n",
        "           show_layer_names=True,\n",
        "           dpi=50,\n",
        "           #rankdir=\"LR\" #LR:plotea horizontal, TB:plotea hacia abajo (por defecto)\n",
        ")"
      ],
      "metadata": {
        "id": "Id2Mv9vzwJEY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resumen del modelo muestra que la salida de la última capa convolucional, con forma **(4, 4, 64)**, fue **aplanada** por la capa `Flatten` a un vector de **1024 unidades** antes de pasar por dos capas densas.\n",
        "\n",
        "Este vector 1D fue luego procesado por:\n",
        "\n",
        "- Una capa densa intermedia con **64 neuronas** y activación `ReLU`.\n",
        "- Una **capa de salida con 10 neuronas**, correspondiente a las **10 clases** del conjunto CIFAR-10."
      ],
      "metadata": {
        "id": "rUViSLF3v95a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Compile y entrene el modelo</b></font>\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://i.gifer.com/origin/b4/b4d657e7ef262b88eb5f7ac021edda87.gif\" width=\"80\"/>\n",
        "</p>\n",
        "\n",
        "> ⚠️ <font color=\"red\"><b>Oops... ¿entrenando en CPU?</b></font>\n",
        "\n",
        "Entrenar redes neuronales en CPU puede ser **considerablemente más lento**, especialmente con datasets como CIFAR-10 (imágenes).\n",
        "\n",
        "<p align=\"center\">\n",
        "  <img src=\"https://media.tenor.com/NCp49Vx5_P0AAAAM/spanky-waiting.gif\"/>\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "**Recomendación:** Si estás usando Google Colab, activa la **GPU** y ahorra tiempo:\n",
        "\n",
        "1. Ve a `Entorno de ejecución` → `Cambiar tipo de entorno de ejecución`.\n",
        "2. En \"Acelerador por hardware\", selecciona **GPU T4**.\n",
        "3. Vuelve a ejecutar toooooodas las celdas."
      ],
      "metadata": {
        "id": "MbXk8lYYTo0U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Función de perdida\n",
        "\n",
        "#Parametros para entrenamiento\n",
        "\n",
        "#Entrenamiento\n"
      ],
      "metadata": {
        "id": "Gt6y0f8MTt9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evalúe el modelo"
      ],
      "metadata": {
        "id": "5roVt9OzXA_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualización de resultados de entrenamiento y validación\n",
        "epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "#Grafica accuracy\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, history.history[\"accuracy\"], label='Entrenamiento')\n",
        "plt.plot(epochs, history.history[\"val_accuracy\"], label='Validación')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Precisión')\n",
        "plt.ylim([0.5, 1])\n",
        "plt.title('Precisión por época')\n",
        "plt.legend()\n",
        "\n",
        "#Graficar loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, history.history[\"loss\"], label='Entrenamiento')\n",
        "plt.plot(epochs, history.history[\"val_loss\"], label='Validación')\n",
        "plt.xlabel('Época')\n",
        "plt.ylabel('Pérdida (Loss)')\n",
        "plt.title('Pérdida por época')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GXlPLBrDXIrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images,  test_labels, verbose=2)\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "KQLUL6x7XQzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <b>Evaluación del modelo</b></font>\n",
        "\n",
        "Nuestra sencilla CNN debería haber alcanzado una **precisión en los datos de prueba superior al 70%**, lo cual es un resultado bastante sólido considerando que se construyó con apenas unas pocas líneas de código utilizando la API secuencial de Keras.\n",
        "\n",
        "Esto demuestra que incluso arquitecturas simples pueden lograr **buen rendimiento en tareas básicas de clasificación de imágenes**, como CIFAR-10.\n",
        "\n",
        "\n",
        "\n",
        "### ¿Quieres explorar otro enfoque?\n",
        "## <b></b></font>\n",
        "\n",
        "Para una implementación más flexible y detallada, puedes revisar el ejemplo de TensorFlow:\n",
        "\n",
        "**[TensorFlow 2 Quickstart for Experts](https://www.tensorflow.org/tutorials/quickstart/advanced)**\n",
        "\n",
        "Este enfoque utiliza:\n",
        "\n",
        "- La **API de subclases de Keras**, que permite definir arquitecturas más personalizadas.\n",
        "- El ciclo de entrenamiento manual con **`tf.GradientTape`**, ideal para aprender cómo funciona el entrenamiento paso a paso.\n",
        "\n",
        "Recomendado si buscas mayor control y aprendizaje más profundo sobre el funcionamiento interno de TensorFlow."
      ],
      "metadata": {
        "id": "9vy_liUOXjLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Funciones de sklearn para metricas de desempeño\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "L13y8Yo5JUtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Predicción\n",
        "pred_labels = model.predict(test_images)\n",
        "y = np.argmax(pred_labels,axis=1)"
      ],
      "metadata": {
        "id": "RKYi9h6cKKY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Métricas de desempeño\n",
        "cm_cnn = confusion_matrix(test_labels, y)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm_cnn, display_labels=class_names)\n",
        "disp.plot()\n",
        "print(classification_report(test_labels, y))\n",
        "print(\"kappa: \" + str(cohen_kappa_score(test_labels, y)))"
      ],
      "metadata": {
        "id": "NqUYDRV_Jgk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ¡Gran trabajo!"
      ],
      "metadata": {
        "id": "am9Tqo_LXpST"
      }
    }
  ]
}